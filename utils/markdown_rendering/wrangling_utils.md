# Functions for data wrangling
This is a markdown rendering of the `wrangling_utils` module used in the notebooks. It is provided here for user reference. The code can be viewed and downloaded from the github repository.


```
""" wrangling_utils.py

    Utility functions for simple data wrangling, including restricting data to certain regions and interpolating data

"""

import xarray as xr 
import pandas as pd 
import numpy as np 
import numpy.ma as ma 
from scipy.interpolate import griddata
```


```
def restrictRegionally(dataset, regionKeyList): 
    """Restrict dataset to input regions.
    
    Args: 
        dataset (xr Dataset): dataset generated by Load_IS2 notebook
        regionKeyList (list): list of region keys to restrict data to 
        
    Returns: 
        regionalDataset (xr Dataset): dataset with restricted data to input regions
    """
    
    # If the user imputs a DataArray, convert to Dataset 
    if type(dataset) == xr.DataArray: 
        dataset = dataset.to_dataset()
    
    def checkKeys(regionKeyList, regionTbl): 
        """Check that regionKeyList was defined correctly

        Raises: 
            ValueError if regionKeyList was not defined correctly 
            warning if all data was removed from the dataset
        """
        if type(regionKeyList) != list: #raise a ValueError if regionKeyList is not a list 
            raise ValueError('regionKeyList needs to be a list. \nFor example, if you want to restrict data to the Beaufort Sea, define regionKeyList = [13]')
        for key in regionKeyList: 
            if key not in list(regionTbl['key']): 
                raise ValueError('Region key ' + str(key) + ' does not exist in region mask. \n Redefine regionKeyList with key numbers from table')
        if len(regionKeyList) == 0: 
            warnings.warn('You removed all the data from the dataset. Are you sure you wanted to do this? \n If not, make sure the list regionKeyList is not empty and try again. \n If you intended to keep data from all regions, set regionKeyList = list(tbl[\"key\"])')
 
    #create a table of keys and labels
    regionMask = dataset.region_mask.attrs
    regionTbl = pd.DataFrame({'key': regionMask['keys'], 'label': regionMask['labels']})
    
    #call function to check if regionKeyList was defined correctly
    checkKeys(regionKeyList, regionTbl)
    
    #filter elements from the ice thickness DataArray where the region is the desired region
    keysToRemove = [key for key in list(regionTbl['key']) if key not in regionKeyList]
    regionalDataset = dataset.copy()
    for var in dataset.data_vars: 
        regionalVar = regionalDataset[var]
        for key in keysToRemove: 
            try:
                regionalVar = regionalVar.where(regionalVar['region_mask'] != key)
            except: 
                pass
        regionalDataset[var] = regionalVar
    
    #add new attributes describing changes made to the dataset
    labels = [regionTbl[regionTbl['key'] == key]['label'].item() for key in regionKeyList]
    if len(labels) < len(regionTbl['key']): 
        if set(regionKeyList) == set([10,11,12,13,15]): #convert to sets so unordered lists are compared
            regionalDataset.attrs['regions with data'] = 'Inner Arctic'
        else:    
            regionalDataset.attrs['regions with data'] = ('%s' % ', '.join(map(str, labels)))
        print('Regions selected: ' + regionalDataset.attrs['regions with data'])
    else: 
        regionalDataset.attrs['regions with data'] = 'All'
        print('Regions selected: All \nNo regions will be removed')
    
    return regionalDataset
```


```
def is2_interp2d(is2_ds, cdr_da, method="nearest", interp_var="all", suffix="_smoothed", pole_hole_lat=88.25): 
    """ Perform 2D interpolation over geographic coordinates for all ICESat-2 sea ice variables with geographic coordinates in xr.Dataset
    As of 06/02/2021, xarray does not have a 2D interpolation function so this function is built on scipy.interpolate.griddata (https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html)
    This function assumes that the dataset has physical coordinates (i.e. lat,lon) as coordinates but logical coordinates (i.e. (x,y)) as dimensions (http://xarray.pydata.org/en/stable/examples/multidimensional-coords.html)
    
    Args: 
        is2_ds (xr.Dataset): ICESat-2 dataset containing variables to interpolate
        cdr_da (xr.DataArray): NSIDC sea ice concentration. Must contain the same time variable as is2_ds
        method (str,optional): interpolation method (default to "linear", choose from {‘linear’, ‘nearest’, ‘cubic’})
        interp_va (srt or list, optional): variables to interpolate (default to "all", variables with geographic coordinates)
        suffix (str, optional): suffix to add to end of data variable name to indicate that the variable has been interpolated (default to "smoothed")
        pole_hole_lat (float, optional): latitude defining pole hole (default to 88.25); set to 90 to keep pole hole 
    
    Returns: 
        ds_interp_sorted (xr.Dataset): dataset with interpolated variables, in alphabetical order 
    
    """
 
    # Get 2d variables in dataset, ensure that user input was valid and raise errors if not
    def get_2d_vars(is2_ds, interp_var): 
        """ Get 2d variables in dataset for interpolation 
        """
        vars_2d = [var for var in is2_ds.data_vars if set(['latitude','longitude']).issubset(list(is2_ds[var].coords))]
        if len(vars_2d) == 0: 
            raise ValueError("The input dataset does not contain geographic coordinates that can be read. See function documentation for more information.")
        if interp_var == "all": 
            interp_var = vars_2d.copy()
        elif (type(interp_var) == str) and (interp_var in vars_2d): 
            interp_var = list(interp_var)
        return interp_var                          
    interp_var_2d = get_2d_vars(is2_ds, interp_var)

    # Get geographic coordinates
    lats = is2_ds['latitude'].values
    lons = is2_ds['longitude'].values

    # Loop through variables and timesteps and interpolate 
    np_cdr = cdr_da.values
    ds_interp = is2_ds.copy()
    for var in interp_var_2d: 
        var_interp_list = []
        da_var = is2_ds[var]
        for timestep in is2_ds.time.values: 
            da = da_var.sel(time=timestep) 
            np_cdr = cdr_da.sel(time=timestep).values
            np_da = da.values
            np_da = ma.masked_where((np.isnan(np_da)) & (np_cdr > 0.15) & (np_cdr < 1.01), np_da)
            np_interp = griddata((lons[~np_da.mask], lats[~np_da.mask]), # Interpolate
                                  np_da[~np_da.mask].flatten(),
                                  (lons, lats), 
                                  fill_value=np.nan,
                                  method=method)
            da_interp = xr.DataArray(data=np_interp, # convert numpy array --> xr.DataArray
                                     dims=da.dims, 
                                     coords=da.coords,
                                     attrs={**da.attrs,'interpolation':'interpolated from original variable using ' + method + ' interpolation'},
                                     name=da.name)
            da_interp = da_interp.where(lats<=pole_hole_lat, np.nan) # Set pole hole to nan
            da_interp = da_interp.expand_dims("time") # Add time as a dimension. Allows for merging DataArrays 
            var_interp_list.append(da_interp)

        var_interp = xr.merge(var_interp_list) # Merge all timesteps together 
        ds_interp[var+suffix] = var_interp[var] # Add interpolated variables as data variable original dataset. If suffix = "", the interpolated variable will replace the original variable 
        
    ds_interp_sorted = ds_interp[sorted(ds_interp.data_vars)] # Sort data variables by alphabetical order
    return ds_interp_sorted 
```


```
def create_empty_xr_ds(xr_ds, start_date, end_date, freq="MS"):
    """ Create an empty xarray dataset for the date range defined by start date --> end date
    
        Args: 
            xr_ds (xr.Dataset, xr.DataArray): dataset to model dimensions off of 
            start_date (str): date for which to start time dimension (i.e "2021-01")
            end_date (str): date for which to end time dimension (default to "2021-04-01")
            freq (str, optional): freqency to do sampling (default to month start, "MS")
        
        Returns: 
            empty_xr_ds (xr.Dataset): empty dataset with time dimension set to the datetime range define by start date -> end date 
    
    """
    if type(xr_ds) == xr.DataArray: 
        xr_ds = xr_ds.to_dataset()

    months = pd.date_range(start=start_date, end=end_date, freq=freq) # date range with month start frequency 
    xr_ds_nan = xr.Dataset(data_vars=xr_ds.where(np.isnan(xr_ds), np.nan).data_vars) # xr_ds, but with all variables set to nan 
    xr_ds_nan_one_timestep = xr_ds_nan.isel(time=0) # Just grab one timestep
    empty_xr_ds = xr.concat([xr_ds_nan_one_timestep]*len(months), dim="time") # Make a xr.Dataset with desired number of empty months
    empty_xr_ds["time"] = months # Reassign time dimension to desired date range 
    return empty_xr_ds
```
