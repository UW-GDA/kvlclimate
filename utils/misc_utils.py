# -*- coding: utf-8 -*-
# +
""" misc_utils.py

    Utility functions for simple data wrangling, including restricting data to certain regions and interpolating data

"""

import xarray as xr 
import pandas as pd 
import numpy as np 
import numpy.ma as ma 
from scipy.interpolate import griddata


# -

def restrictRegionally(dataset, regionKeyList): 
    """Restrict dataset to input regions.
    
    Args: 
        dataset (xr Dataset): dataset generated by Load_IS2 notebook
        regionKeyList (list): list of region keys to restrict data to 
        
    Returns: 
        regionalDataset (xr Dataset): dataset with restricted data to input regions
    """
    
    # If the user imputs a DataArray, convert to Dataset 
    if type(dataset) == xr.DataArray: 
        dataset = dataset.to_dataset()
    
    def checkKeys(regionKeyList, regionTbl): 
        """Check that regionKeyList was defined correctly

        Raises: 
            ValueError if regionKeyList was not defined correctly 
            warning if all data was removed from the dataset
        """
        if type(regionKeyList) != list: #raise a ValueError if regionKeyList is not a list 
            raise ValueError('regionKeyList needs to be a list. \nFor example, if you want to restrict data to the Beaufort Sea, define regionKeyList = [13]')
        for key in regionKeyList: 
            if key not in list(regionTbl['key']): 
                raise ValueError('Region key ' + str(key) + ' does not exist in region mask. \n Redefine regionKeyList with key numbers from table')
        if len(regionKeyList) == 0: 
            warnings.warn('You removed all the data from the dataset. Are you sure you wanted to do this? \n If not, make sure the list regionKeyList is not empty and try again. \n If you intended to keep data from all regions, set regionKeyList = list(tbl[\"key\"])')
 
    #create a table of keys and labels
    regionMask = dataset.region_mask.attrs
    regionTbl = pd.DataFrame({'key': regionMask['keys'], 'label': regionMask['labels']})
    
    #call function to check if regionKeyList was defined correctly
    checkKeys(regionKeyList, regionTbl)
    
    #filter elements from the ice thickness DataArray where the region is the desired region
    keysToRemove = [key for key in list(regionTbl['key']) if key not in regionKeyList]
    regionalDataset = dataset.copy()
    for var in dataset.data_vars: 
        regionalVar = regionalDataset[var]
        for key in keysToRemove: 
            try:
                regionalVar = regionalVar.where(regionalVar['region_mask'] != key)
            except: 
                pass
        regionalDataset[var] = regionalVar
    
    #add new attributes describing changes made to the dataset
    labels = [regionTbl[regionTbl['key'] == key]['label'].item() for key in regionKeyList]
    if len(labels) < len(regionTbl['key']): 
        if set(regionKeyList) == set([10,11,12,13,15]): #convert to sets so unordered lists are compared
            regionalDataset.attrs['regions with data'] = 'Inner Arctic'
        else:    
            regionalDataset.attrs['regions with data'] = ('%s' % ', '.join(map(str, labels)))
        print('Regions selected: ' + regionalDataset.attrs['regions with data'])
    else: 
        regionalDataset.attrs['regions with data'] = 'All'
        print('Regions selected: All \nNo regions will be removed')
    
    return regionalDataset


def is2_interp2d(is2_ds, cdr_da, method="nearest", interp_var="all", suffix="_smoothed"): 
    """ Perform 2D interpolation over geographic coordinates for all ICESat-2 sea ice variables with geographic coordinates in xr.Dataset
    As of 06/02/2021, xarray does not have a 2D interpolation function so this function is built on scipy.interpolate.griddata (https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html)
    This function assumes that the dataset has physical coordinates (i.e. lat,lon) as coordinates but logical coordinates (i.e. (x,y)) as dimensions (http://xarray.pydata.org/en/stable/examples/multidimensional-coords.html)
    
    Args: 
        is2_ds (xr.Dataset): ICESat-2 dataset containing variables to interpolate
        cdr_da (xr.DataArray): NSIDC sea ice concentration. Must contain the same time variable as is2_ds
        method (str,optional): interpolation method (default to "linear", choose from {‘linear’, ‘nearest’, ‘cubic’})
        interp_va (srt or list, optional): variables to interpolate (default to "all", variables with geographic coordinates)
        suffix (str, optional): suffix to add to end of data variable name to indicate that the variable has been interpolated (default to "smoothed")
    
    Returns: 
        ds_interp_sorted (xr.Dataset): dataset with interpolated variables, in alphabetical order 
    
    """
 
    # Get 2d variables in dataset, ensure that user input was valid and raise errors if not
    def get_2d_vars(is2_ds, interp_var): 
        """ Get 2d variables in dataset for interpolation 
        """
        vars_2d = [var for var in is2_ds.data_vars if set(['latitude','longitude']).issubset(list(is2_ds[var].coords))]
        if len(vars_2d) == 0: 
            raise ValueError("The input dataset does not contain geographic coordinates that can be read. See function documentation for more information.")
        if interp_var == "all": 
            interp_var = vars_2d.copy()
        elif (type(interp_var) == str) and (interp_var in vars_2d): 
            interp_var = list(interp_var)
        return interp_var                          
    interp_var_2d = get_2d_vars(is2_ds, interp_var)

    # Get geographic coordinates
    lats = is2_ds['latitude'].values
    lons = is2_ds['longitude'].values

    # Loop through variables and timesteps and interpolate 
    np_cdr = cdr_da.values
    ds_interp = is2_ds.copy()
    for var in interp_var_2d: 
        var_interp_list = []
        da_var = is2_ds[var]
        for timestep in is2_ds.time.values: 
            da = da_var.sel(time=timestep) 
            np_cdr = cdr_da.sel(time=timestep).values
            np_da = da.values
            np_da = ma.masked_where((np.isnan(np_da)) & (np_cdr > 0.15) & (np_cdr < 1.01), np_da)
            np_interp = griddata((lons[~np_da.mask], lats[~np_da.mask]), # Interpolate
                                  np_da[~np_da.mask].flatten(),
                                  (lons, lats), 
                                  fill_value=np.nan,
                                  method=method)
            da_interp = xr.DataArray(data=np_interp, # convert numpy array --> xr.DataArray
                                     dims=da.dims, 
                                     coords=da.coords,
                                     attrs={**da.attrs,'interpolation':'interpolated from original variable using ' + method + ' interpolation'},
                                     name=da.name)
            da_interp = da_interp.where(lats<88, np.nan) # Set pole hole to nan
            da_interp = da_interp.expand_dims("time") # Add time as a dimension. Allows for merging DataArrays 
            var_interp_list.append(da_interp)

        var_interp = xr.merge(var_interp_list) # Merge all timesteps together 
        ds_interp[var+suffix] = var_interp[var] # Add interpolated variables as data variable original dataset. If suffix = "", the interpolated variable will replace the original variable 
        
    ds_interp_sorted = ds_interp[sorted(ds_interp.data_vars)] # Sort data variables by alphabetical order
    return ds_interp_sorted 


def create_empty_xr_ds(xr_ds, start_date, end_date, freq="MS"):
    """ Create an empty xarray dataset for the date range defined by start date --> end date
    
        Args: 
            xr_ds (xr.Dataset, xr.DataArray): dataset to model dimensions off of 
            start_date (str): date for which to start time dimension (i.e "2021-01")
            end_date (str): date for which to end time dimension (default to "2021-04-01")
            freq (str, optional): freqency to do sampling (default to month start, "MS")
        
        Returns: 
            empty_xr_ds (xr.Dataset): empty dataset with time dimension set to the datetime range define by start date -> end date 
    
    """
    if type(xr_ds) == xr.DataArray: 
        xr_ds = xr_ds.to_dataset()

    months = pd.date_range(start=start_date, end=end_date, freq=freq) # date range with month start frequency 
    xr_ds_nan = xr.Dataset(data_vars=xr_ds.where(np.isnan(xr_ds), np.nan).data_vars) # xr_ds, but with all variables set to nan 
    xr_ds_nan_one_timestep = xr_ds_nan.isel(time=0) # Just grab one timestep
    empty_xr_ds = xr.concat([xr_ds_nan_one_timestep]*len(months), dim="time") # Make a xr.Dataset with desired number of empty months
    empty_xr_ds["time"] = months # Reassign time dimension to desired date range 
    return empty_xr_ds


def compute_winter_means(da, start_year=None, end_year=None, x_dim="x", y_dim="y", mask_nans=True): 
    """ Compute winter mean monthly values by horizontal dimensions 
    Winter is defined by the months Sep --> Apr 
    
    Args: 
        da (xr.DataArray): DataArray to compute mean values. Must contain a time dimension and two horizontal dimensions 
        start_year (int, optional): Year to start computing means for (default to first year in da)
        end_year (int, optional): Year to end computing means for (default to last year in da)
        x_dim (str, optional): Name of x dimension in da (default to "x")
        y_dim (str, optional): Name of y dimension in da (default to "y")
        mask_nans (bool, optional): Mask nans across the same months through each winter season? I.e. if Dec 2020 is missing data in the Laptev Sea, mask out missing gridcells in the Laptev Sea for December for all years (default to True)
        
    Returns: 
        df_means (pd.DataFrame): table of means with months Sep --> Apr as column values, Winter name (i.e. "Winter 2019-20") as columns
        returns None if no data found in the time range to compute means for 

    """
    
    # Get start and end year 
    if start_year is None: 
        start_year = pd.to_datetime(da.time.values[0]).year 
    if end_year is None: 
        end_year = pd.to_datetime(da.time.values[-1]).year

    # If winter season is missing data for any months, fill it with an empty DataArray
    # I wrote this because sep and oct 2018 don't have any data! 
    winter_months = [] # Empty list to store timestamps corresponding with months in a winter season 
    curr_year = int(start_year) 
    while curr_year < end_year: 
        winter_n = getWinterDateRange(start_year=str(curr_year), end_year=str(curr_year+1), start_month="September", end_month="April")
        winter_months.append(winter_n)
        missing_months = [time for time in winter_n if time not in da.time.values]
        for missing_month in missing_months: 
            #print("No data for "+missing_month.strftime("%Y-%m-%d")+"...created empty DataArray to fill timestep")
            empty_da = create_empty_xr_ds(xr_ds=da, start_date=missing_month, end_date=missing_month)[da.name] # Create emmpty DataArray. Function returns an xr.Dataset, so I convert to DataArray by grabbing the variable name 
            da = xr.concat([empty_da, da], dim="time") # Add to original dataframe
        curr_year+=1

    # Set to nan any cells that are nan in any other month 
    # We just want to compare grid cells that have data in all years for a given month 
    # This avoids one winter having an inflated mean due to regional differences in data coverage
    if mask_nans==True: 
        da_list = []
        for mon in [9,10,11,12,1,2,3,4]: 
            # Get all months in da that have month mon_sel; i.e. grab all Decemeber months (Dec 2018, Dec 2019, Dec 2010, etc)
            month_timesteps_all = [time for time in pd.to_datetime(da.time.values) if time.month==mon] 
            da_mon = da.sel(time=month_timesteps_all)

            for mon_yr in month_timesteps_all: 
                if np.isnan(da_mon.sel(time=mon_yr).mean(dim=[x_dim,y_dim]).values.item()): # If no data in that month, ignore it, since the mean function will set it to nan anyways 
                    pass 
                else: 
                    da_mon = da_mon.where(~np.isnan(da_mon.sel(time=mon_yr)), np.nan)

                da_list.append(da_mon)
        da = xr.merge(da_list)[da.name]
    else:
        pass

    means_da = da.mean(dim=[x_dim,y_dim])

    # Put data in a pd.DataFrame object for easy plotting 
    df_means = pd.DataFrame(index=["Sep","Oct","Nov","Dec","Jan","Feb","Mar","Apr"])
    for winter in winter_months: 
        winter_str = "Winter "+str(winter[0].year)+"-"+str(winter[-1].year)[2:] # i.e. "Winter 2020-21"
        df_means[winter_str] = means_da.sel(time=winter).values

    df_means.columns.name = "time period"
    df_means.index.name = "month"
    if len(df_means.T) == 0: # Return None if df is empty 
        return None
    else: 
        return df_means 


def getWinterDateRange(start_year, end_year, start_month="September", end_month="April"): 
    """ Gets date range for winter season/s
    Calling the function for start_year=2018, end_year=2020, start_month="November", end_month="April" will generate a date range from Nov 2018-Apr 2019 and Nov 2019-Apr 2020
    
    Args: 
        start_year (str): start year 
        end_year (str): end year 
        start_month (str, optional): month at which winter starts (default to November)
        end_month (str, optional): month at which winter ends (default to April)
        
    Returns: 
        winters (list): list of dates for all winter seasons in the input range (i.e: ['1980-11','1980-12','1981-01',
         '1981-02','1981-03','1981-04')
    """
    start_year = int(start_year)
    end_year = int(end_year)
    
    winters = []
    for year in range(start_year, end_year, 1):
        winters += pd.date_range(start = str(year)+'-'+start_month,
                                 end = str(year+1)+'-'+end_month,
                                 freq = 'MS')
    winters = pd.to_datetime(winters)
    return winters
